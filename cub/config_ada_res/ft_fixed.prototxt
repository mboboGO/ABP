layer {
  name: "data"
  type: "Input"
  top: "layer_512_3_sum"
  input_param {
    shape {
      dim: 32
      dim: 2048
      dim: 14
      dim: 14
    }
  }
}
layer {
  name: "label"
  type: "Input"
  top: "label"
  input_param {
    shape {
      dim: 32
    }
  }
}
layer {
  name: "top_bn"
  type: "BatchNorm"
  bottom: "layer_512_3_sum"
  top: "top_bn"
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  param {
    lr_mult: 0.0
  }
  batch_norm_param {
    moving_average_fraction: 0.996999979019
  }
}
layer {
  name: "top_scale"
  type: "Scale"
  bottom: "top_bn"
  top: "top_scale"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 2.0
    decay_mult: 1.0
  }
  scale_param {
    bias_term: true
  }
}
layer {
  name: "p/gpooling"
  type: "Pooling"
  bottom: "top_scale"
  top: "p/gpooling"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "p/conv"
  type: "Convolution"
  bottom: "p/gpooling"
  top: "p/conv"
  param {
    lr_mult: 9.99999974738e-05
    decay_mult: 1.0
  }
  param {
    lr_mult: 0.000199999994948
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 1
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.0010000000475
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "p/relu"
  type: "ReLU"
  bottom: "p/conv"
  top: "p/relu"
}
layer {
  name: "p/att"
  type: "Attention"
  bottom: "top_scale"
  bottom: "p/relu"
  top: "p/att"
  attention_param {
    operation: EXP
  }
}
layer {
  name: "s/conv"
  type: "Convolution"
  bottom: "top_scale"
  top: "s/conv"
  param {
    lr_mult: 9.99999974738e-05
    decay_mult: 1.0
  }
  param {
    lr_mult: 0.000199999994948
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 1
    kernel_size: 1
    weight_filler {
      type: "gaussian"
      std: 0.0010000000475
    }
    bias_filler {
      type: "constant"
      value: 0.0
    }
  }
}
layer {
  name: "s/reshape1"
  type: "Reshape"
  bottom: "s/conv"
  top: "s/reshape1"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 1
      dim: -1
    }
  }
}
layer {
  name: "s/softmax"
  type: "Softmax"
  bottom: "s/reshape1"
  top: "s/softmax"
  softmax_param {
    axis: -1
  }
}
layer {
  name: "s/reshape2"
  type: "Reshape"
  bottom: "s/softmax"
  top: "s/reshape2"
  reshape_param {
    shape {
      dim: 0
      dim: 0
      dim: 14
      dim: 14
    }
  }
}
layer {
  name: "s/att"
  type: "Attention"
  bottom: "top_scale"
  bottom: "s/reshape2"
  top: "s/att"
  attention_param {
    operation: SPACE
  }
}
layer {
  name: "outer_product"
  type: "CompactBilinear"
  bottom: "p/att"
  bottom: "s/att"
  top: "outer_product"
  compact_bilinear_param {
    num_output: 8192
  }
}
layer {
  name: "root"
  type: "SignedPower"
  bottom: "outer_product"
  top: "root"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  power_param {
    power: 0.5
  }
}
layer {
  name: "l2"
  type: "L2Normalize"
  bottom: "root"
  top: "l2"
}
layer {
  name: "fc8_ft"
  type: "InnerProduct"
  bottom: "l2"
  top: "fc8_ft"
  param {
    lr_mult: 10.0
    decay_mult: 1.0
  }
  param {
    lr_mult: 20.0
    decay_mult: 0.0
  }
  inner_product_param {
    num_output: 201
    weight_filler {
      std: 0.0010000000475
    }
    bias_filler {
      value: 0.0
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8_ft"
  bottom: "label"
  top: "loss"
  loss_weight: 1.0
}
layer {
  name: "Accuracy"
  type: "Accuracy"
  bottom: "fc8_ft"
  bottom: "label"
  top: "Accuracy"
  include {
    phase: TEST
  }
}
